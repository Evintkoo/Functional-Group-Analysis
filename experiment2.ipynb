{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "# general tools\n",
    "import numpy as np\n",
    "\n",
    "# RDkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "\n",
    "# Pytorch and Pytorch Geometric\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split \n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(x, permitted_list):\n",
    "    \"\"\"\n",
    "    Maps input elements x which are not in the permitted list to the last element\n",
    "    of the permitted list.\n",
    "    \"\"\"\n",
    "\n",
    "    if x not in permitted_list:\n",
    "        x = permitted_list[-1]\n",
    "\n",
    "    binary_encoding = [int(boolean_value) for boolean_value in list(map(lambda s: x == s, permitted_list))]\n",
    "\n",
    "    return binary_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atom_features(atom, \n",
    "                      use_chirality = True, \n",
    "                      hydrogens_implicit = False):\n",
    "    \"\"\"\n",
    "    Takes an RDKit atom object as input and gives a 1d-numpy array of atom features as output.\n",
    "    \"\"\"\n",
    "\n",
    "    # define list of permitted atoms\n",
    "    \n",
    "    permitted_list_of_atoms =  ['C','N','O','S','F','Si','P','Cl','Br','Mg','Na','Ca','Fe','As','Al','I', 'B','V','K','Tl','Yb','Sb','Sn','Ag','Pd','Co','Se','Ti','Zn', 'Li','Ge','Cu','Au','Ni','Cd','In','Mn','Zr','Cr','Pt','Hg','Pb','Unknown']\n",
    "    \n",
    "    if hydrogens_implicit == False:\n",
    "        permitted_list_of_atoms = ['H'] + permitted_list_of_atoms\n",
    "    \n",
    "    # compute atom features\n",
    "    \n",
    "    atom_type_enc = one_hot_encoding(str(atom.GetSymbol()), permitted_list_of_atoms)\n",
    "    \n",
    "    n_heavy_neighbors_enc = one_hot_encoding(int(atom.GetDegree()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
    "    \n",
    "    formal_charge_enc = one_hot_encoding(int(atom.GetFormalCharge()), [-3, -2, -1, 0, 1, 2, 3, \"Extreme\"])\n",
    "    \n",
    "    hybridisation_type_enc = one_hot_encoding(str(atom.GetHybridization()), [\"S\", \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\", \"OTHER\"])\n",
    "    \n",
    "    is_in_a_ring_enc = [int(atom.IsInRing())]\n",
    "    \n",
    "    is_aromatic_enc = [int(atom.GetIsAromatic())]\n",
    "    \n",
    "    atomic_mass_scaled = [float((atom.GetMass() - 10.812)/116.092)]\n",
    "    \n",
    "    vdw_radius_scaled = [float((Chem.GetPeriodicTable().GetRvdw(atom.GetAtomicNum()) - 1.5)/0.6)]\n",
    "    \n",
    "    covalent_radius_scaled = [float((Chem.GetPeriodicTable().GetRcovalent(atom.GetAtomicNum()) - 0.64)/0.76)]\n",
    "    \n",
    "    #atom_feature_vector = atom_type_enc + n_heavy_neighbors_enc + formal_charge_enc + hybridisation_type_enc + is_in_a_ring_enc + is_aromatic_enc + atomic_mass_scaled + vdw_radius_scaled + covalent_radius_scaled\n",
    "    atom_feature_vector = atom_type_enc + hybridisation_type_enc + is_in_a_ring_enc + is_aromatic_enc + atomic_mass_scaled \n",
    "    print(permitted_list_of_atoms)\n",
    "    if use_chirality == True:\n",
    "        chirality_type_enc = one_hot_encoding(str(atom.GetChiralTag()), [\"CHI_UNSPECIFIED\", \"CHI_TETRAHEDRAL_CW\", \"CHI_TETRAHEDRAL_CCW\", \"CHI_OTHER\"])\n",
    "        atom_feature_vector += chirality_type_enc\n",
    "    \n",
    "    if hydrogens_implicit == True:\n",
    "        n_hydrogens_enc = one_hot_encoding(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
    "        atom_feature_vector += n_hydrogens_enc\n",
    "\n",
    "    return np.array(atom_feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bond_features(bond, \n",
    "                      use_stereochemistry = True):\n",
    "    \"\"\"\n",
    "    Takes an RDKit bond object as input and gives a 1d-numpy array of bond features as output.\n",
    "    \"\"\"\n",
    "\n",
    "    permitted_list_of_bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE, Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "\n",
    "    bond_type_enc = one_hot_encoding(bond.GetBondType(), permitted_list_of_bond_types)\n",
    "    \n",
    "    bond_is_conj_enc = [int(bond.GetIsConjugated())]\n",
    "    \n",
    "    bond_is_in_ring_enc = [int(bond.IsInRing())]\n",
    "    \n",
    "    bond_feature_vector = bond_type_enc + bond_is_conj_enc + bond_is_in_ring_enc\n",
    "    \n",
    "    if use_stereochemistry == True:\n",
    "        stereo_type_enc = one_hot_encoding(str(bond.GetStereo()), [\"STEREOZ\", \"STEREOE\", \"STEREOANY\", \"STEREONONE\"])\n",
    "        bond_feature_vector += stereo_type_enc\n",
    "\n",
    "    return np.array(bond_feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"250k_rndm_zinc_drugs_clean_3.csv\")\n",
    "smiles = df[\"smiles\"]\n",
    "y = df[\"qed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr', 'Pt', 'Hg', 'Pb', 'Unknown']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.01032802, 1.        ,\n",
       "       0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol = Chem.MolFromSmiles(smiles[0])\n",
    "get_atom_features(mol.GetAtomWithIdx(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_smiles_to_matrix(smiles):\n",
    "    matrix = []\n",
    "    for smile in tqdm(smiles):\n",
    "        #Chem.MolFromSmiles(smiles[0]).GetBondBetweenAtoms(0, len()).GetSymbol()\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        total_atoms = len(mol.GetAtoms())-1\n",
    "        # atom hotencoding\n",
    "        atom_encoding = []\n",
    "        for i in range(total_atoms):\n",
    "            if len(atom_encoding) > 0:\n",
    "                atom_encoding += get_atom_features(mol.GetAtomWithIdx(i))\n",
    "            else:\n",
    "                atom_encoding = get_atom_features(mol.GetAtomWithIdx(i))\n",
    "        bond_encoding = []\n",
    "        for bonds in mol.GetBonds():\n",
    "            if len(bond_encoding) > 0:\n",
    "                bond_encoding += get_bond_features(bonds)\n",
    "            else :\n",
    "                bond_encoding = get_bond_features(bonds)\n",
    "        data = list(atom_encoding) + list(bond_encoding)\n",
    "        matrix.append(data)\n",
    "    matrix = np.array(matrix)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/249455 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249455/249455 [03:08<00:00, 1321.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(249455, 68)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = convert_smiles_to_matrix(smiles)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ANN model for tokenizing with specific input and output sizes using Sequential\n",
    "class TokenizerANN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \n",
    "        super(TokenizerANN, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),  # First hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),       # Third hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),       # Fourth hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),        # Fifth hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_size) ,\n",
    "            nn.Softmax()# Encoder output layer\n",
    "        )\n",
    "       \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_size, 128),          # Decoder input layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),        # First hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),       # Second hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),       # Third hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_size)   # Decoder output layer\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 32, 256]          17,664\n",
      "              ReLU-2              [-1, 32, 256]               0\n",
      "            Linear-3              [-1, 32, 256]          65,792\n",
      "              ReLU-4              [-1, 32, 256]               0\n",
      "            Linear-5              [-1, 32, 128]          32,896\n",
      "              ReLU-6              [-1, 32, 128]               0\n",
      "            Linear-7              [-1, 32, 128]          16,512\n",
      "              ReLU-8              [-1, 32, 128]               0\n",
      "            Linear-9                [-1, 32, 4]             516\n",
      "          Softmax-10                [-1, 32, 4]               0\n",
      "           Linear-11              [-1, 32, 128]             640\n",
      "             ReLU-12              [-1, 32, 128]               0\n",
      "           Linear-13              [-1, 32, 128]          16,512\n",
      "             ReLU-14              [-1, 32, 128]               0\n",
      "           Linear-15              [-1, 32, 256]          33,024\n",
      "             ReLU-16              [-1, 32, 256]               0\n",
      "           Linear-17              [-1, 32, 256]          65,792\n",
      "             ReLU-18              [-1, 32, 256]               0\n",
      "           Linear-19               [-1, 32, 68]          17,476\n",
      "================================================================\n",
      "Total params: 266,824\n",
      "Trainable params: 266,824\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.77\n",
      "Params size (MB): 1.02\n",
      "Estimated Total Size (MB): 1.79\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Evint\\Documents\\Projects\\Functional-Group-Analysis\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "batch_size = 32\n",
    "target_size = 4\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 150\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, X, \n",
    "                                   random_state=104,  \n",
    "                                   test_size=0.4,  \n",
    "                                   shuffle=True) \n",
    "\n",
    "tensor_X_train = torch.from_numpy(X_train).to(device)\n",
    "tensor_X_test = torch.from_numpy(X_test).to(device)\n",
    "tensor_y_train = torch.from_numpy(y_train).to(device)\n",
    "tensor_y_test = torch.from_numpy(y_test).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(tensor_X_train, tensor_y_train)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "model = TokenizerANN(input_size = tensor_X_test.shape[1],\n",
    "                     output_size=target_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# define history list\n",
    "class history:\n",
    "    def __init__(self) -> None:\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        self.metric_loss = []\n",
    "        self.val_metric_loss = []\n",
    "h = history()\n",
    "\n",
    "summary(model, (batch_size, tensor_X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Evint\\AppData\\Local\\Temp\\ipykernel_38116\\3657820513.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = model(torch.tensor(inputs, dtype=torch.float32))\n",
      "C:\\Users\\Evint\\AppData\\Local\\Temp\\ipykernel_38116\\3657820513.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_outputs = model(torch.tensor(tensor_X_test, dtype=torch.float32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/350] - Loss: 1.9349; val_loss: 0.4068 saved\n",
      "Epoch [2/350] - Loss: 0.2917; val_loss: 0.2156 saved\n",
      "Epoch [3/350] - Loss: 0.2043; val_loss: 0.2012 saved\n",
      "Epoch [4/350] - Loss: 0.1995; val_loss: 0.1955 saved\n",
      "Epoch [5/350] - Loss: 0.1960; val_loss: 0.1937 saved\n",
      "Epoch [6/350] - Loss: 0.1919; val_loss: 0.1890 saved\n",
      "Epoch [7/350] - Loss: 0.1871; val_loss: 0.1838 saved\n",
      "Epoch [8/350] - Loss: 0.1817; val_loss: 0.1795 saved\n",
      "Epoch [9/350] - Loss: 0.1762; val_loss: 0.1716 saved\n",
      "Epoch [10/350] - Loss: 0.1716; val_loss: 0.1704 saved\n",
      "Epoch [11/350] - Loss: 0.1675; val_loss: 0.1704 saved\n",
      "Epoch [12/350] - Loss: 0.1641; val_loss: 0.1593 saved\n",
      "Epoch [13/350] - Loss: 0.1609; val_loss: 0.1573 saved\n",
      "Epoch [14/350] - Loss: 0.1596; val_loss: 0.1564 saved\n",
      "Epoch [15/350] - Loss: 0.1564; val_loss: 0.1544 saved\n",
      "Epoch [16/350] - Loss: 0.1543; val_loss: 0.1686 saved\n",
      "Epoch [17/350] - Loss: 0.1507; val_loss: 0.1489 saved\n",
      "Epoch [18/350] - Loss: 0.1479; val_loss: 0.1454 saved\n",
      "Epoch [19/350] - Loss: 0.1460; val_loss: 0.1460 saved\n",
      "Epoch [20/350] - Loss: 0.1440; val_loss: 0.1462 saved\n",
      "Epoch [21/350] - Loss: 0.1427; val_loss: 0.1438 saved\n",
      "Epoch [22/350] - Loss: 0.1418; val_loss: 0.1407 saved\n",
      "Epoch [23/350] - Loss: 0.1410; val_loss: 0.1394 saved\n",
      "Epoch [24/350] - Loss: 0.1399; val_loss: 0.1386 saved\n",
      "Epoch [25/350] - Loss: 0.1393; val_loss: 0.1386 saved\n",
      "Epoch [26/350] - Loss: 0.1382; val_loss: 0.1410 saved\n",
      "Epoch [27/350] - Loss: 0.1374; val_loss: 0.1380 saved\n",
      "Epoch [28/350] - Loss: 0.1362; val_loss: 0.1356 saved\n",
      "Epoch [29/350] - Loss: 0.1355; val_loss: 0.1355 saved\n",
      "Epoch [30/350] - Loss: 0.1346; val_loss: 0.1335 saved\n",
      "Epoch [31/350] - Loss: 0.1340; val_loss: 0.1330 saved\n",
      "Epoch [32/350] - Loss: 0.1335; val_loss: 0.1330 saved\n",
      "Epoch [33/350] - Loss: 0.1330; val_loss: 0.1316 saved\n",
      "Epoch [34/350] - Loss: 0.1326; val_loss: 0.1339 saved\n",
      "Epoch [35/350] - Loss: 0.1320; val_loss: 0.1308 saved\n",
      "Epoch [36/350] - Loss: 0.1321; val_loss: 0.1326 \n",
      "Epoch [37/350] - Loss: 0.1323; val_loss: 0.1316 \n",
      "Epoch [38/350] - Loss: 0.1308; val_loss: 0.1324 saved\n",
      "Epoch [39/350] - Loss: 0.1310; val_loss: 0.1321 \n",
      "Epoch [40/350] - Loss: 0.1310; val_loss: 0.1292 \n",
      "Epoch [41/350] - Loss: 0.1326; val_loss: 0.1310 \n",
      "Epoch [42/350] - Loss: 0.1300; val_loss: 0.1289 saved\n",
      "Epoch [43/350] - Loss: 0.1297; val_loss: 0.1306 saved\n",
      "Epoch [44/350] - Loss: 0.1296; val_loss: 0.1298 saved\n",
      "Epoch [45/350] - Loss: 0.1291; val_loss: 0.1299 saved\n",
      "Epoch [46/350] - Loss: 0.1299; val_loss: 0.1284 \n",
      "Epoch [47/350] - Loss: 0.1288; val_loss: 0.1293 saved\n",
      "Epoch [48/350] - Loss: 0.1290; val_loss: 0.1295 \n",
      "Epoch [49/350] - Loss: 0.1302; val_loss: 0.1293 \n",
      "Epoch [50/350] - Loss: 0.1290; val_loss: 0.1304 \n",
      "Epoch [51/350] - Loss: 0.1281; val_loss: 0.1291 saved\n",
      "Epoch [52/350] - Loss: 0.1287; val_loss: 0.1271 \n",
      "Epoch [53/350] - Loss: 0.1279; val_loss: 0.1284 saved\n",
      "Epoch [54/350] - Loss: 0.1282; val_loss: 0.1276 \n",
      "Epoch [55/350] - Loss: 0.1282; val_loss: 0.1275 \n",
      "Epoch [56/350] - Loss: 0.1277; val_loss: 0.1294 saved\n",
      "Epoch [57/350] - Loss: 0.1275; val_loss: 0.1288 saved\n",
      "Epoch [58/350] - Loss: 0.1281; val_loss: 0.1280 \n",
      "Epoch [59/350] - Loss: 0.1271; val_loss: 0.1267 saved\n",
      "Epoch [60/350] - Loss: 0.1270; val_loss: 0.1275 saved\n",
      "Epoch [61/350] - Loss: 0.1267; val_loss: 0.1268 saved\n",
      "Epoch [62/350] - Loss: 0.1267; val_loss: 0.1263 saved\n",
      "Epoch [63/350] - Loss: 0.1265; val_loss: 0.1270 saved\n",
      "Epoch [64/350] - Loss: 0.1264; val_loss: 0.1272 saved\n",
      "Epoch [65/350] - Loss: 0.1261; val_loss: 0.1660 saved\n",
      "Epoch [66/350] - Loss: 0.1262; val_loss: 0.1290 \n",
      "Epoch [67/350] - Loss: 0.1259; val_loss: 0.1259 saved\n",
      "Epoch [68/350] - Loss: 0.1259; val_loss: 0.1251 \n",
      "Epoch [69/350] - Loss: 0.1255; val_loss: 0.1254 saved\n",
      "Epoch [70/350] - Loss: 0.1257; val_loss: 0.1282 \n",
      "Epoch [71/350] - Loss: 0.1257; val_loss: 0.1265 \n",
      "Epoch [72/350] - Loss: 0.1251; val_loss: 0.1282 saved\n",
      "Epoch [73/350] - Loss: 0.1251; val_loss: 0.1245 saved\n",
      "Epoch [74/350] - Loss: 0.1255; val_loss: 0.1241 \n",
      "Epoch [75/350] - Loss: 0.1246; val_loss: 0.1242 saved\n",
      "Epoch [76/350] - Loss: 0.1251; val_loss: 0.1245 \n",
      "Epoch [77/350] - Loss: 0.1246; val_loss: 0.1236 saved\n",
      "Epoch [78/350] - Loss: 0.1314; val_loss: 0.1550 \n",
      "Epoch [79/350] - Loss: 0.1429; val_loss: 0.1294 \n",
      "Epoch [80/350] - Loss: 0.1272; val_loss: 0.1241 \n",
      "Epoch [81/350] - Loss: 0.1366; val_loss: 0.1363 \n",
      "Epoch [82/350] - Loss: 0.1351; val_loss: 0.1593 \n",
      "Epoch [83/350] - Loss: 0.1375; val_loss: 0.1536 \n",
      "Epoch [84/350] - Loss: 0.1424; val_loss: 0.1399 \n",
      "Epoch [85/350] - Loss: 0.1387; val_loss: 0.1389 \n",
      "Epoch [86/350] - Loss: 0.1367; val_loss: 0.1365 \n",
      "Epoch [87/350] - Loss: 0.1348; val_loss: 0.1346 \n",
      "Epoch [88/350] - Loss: 0.1335; val_loss: 0.1327 \n",
      "Epoch [89/350] - Loss: 0.1318; val_loss: 0.1301 \n",
      "Epoch [90/350] - Loss: 0.1316; val_loss: 0.1317 \n",
      "Epoch [91/350] - Loss: 0.1300; val_loss: 0.1287 \n",
      "Epoch [92/350] - Loss: 0.1289; val_loss: 0.1281 \n",
      "Epoch [93/350] - Loss: 0.1284; val_loss: 0.1277 \n",
      "Epoch [94/350] - Loss: 0.1278; val_loss: 0.1276 \n",
      "Epoch [95/350] - Loss: 0.1288; val_loss: 0.1282 \n",
      "Epoch [96/350] - Loss: 0.1271; val_loss: 0.1289 \n",
      "Epoch [97/350] - Loss: 0.1266; val_loss: 0.1271 \n",
      "Epoch [98/350] - Loss: 0.1262; val_loss: 0.1260 \n",
      "Epoch [99/350] - Loss: 0.1259; val_loss: 0.1251 \n",
      "Epoch [100/350] - Loss: 0.1257; val_loss: 0.1268 \n",
      "Epoch [101/350] - Loss: 0.1249; val_loss: 0.1255 \n",
      "Epoch [102/350] - Loss: 0.1251; val_loss: 0.1253 \n",
      "Epoch [103/350] - Loss: 0.1245; val_loss: 0.1246 saved\n",
      "Epoch [104/350] - Loss: 0.1241; val_loss: 0.1253 saved\n",
      "Epoch [105/350] - Loss: 0.1240; val_loss: 0.1238 saved\n",
      "Epoch [106/350] - Loss: 0.1237; val_loss: 0.1235 saved\n",
      "Epoch [107/350] - Loss: 0.1234; val_loss: 0.1227 saved\n",
      "Epoch [108/350] - Loss: 0.1243; val_loss: 0.1245 \n",
      "Epoch [109/350] - Loss: 0.1230; val_loss: 0.1234 saved\n",
      "Epoch [110/350] - Loss: 0.1231; val_loss: 0.1223 \n",
      "Epoch [111/350] - Loss: 0.1234; val_loss: 0.1239 \n",
      "Epoch [112/350] - Loss: 0.1236; val_loss: 0.1221 \n",
      "Epoch [113/350] - Loss: 0.1231; val_loss: 0.1241 \n",
      "Epoch [114/350] - Loss: 0.1227; val_loss: 0.1240 saved\n",
      "Epoch [115/350] - Loss: 0.1220; val_loss: 0.1215 saved\n",
      "Epoch [116/350] - Loss: 0.1226; val_loss: 0.1224 \n",
      "Epoch [117/350] - Loss: 0.1225; val_loss: 0.1220 \n",
      "Epoch [118/350] - Loss: 0.1216; val_loss: 0.1210 saved\n",
      "Epoch [119/350] - Loss: 0.1214; val_loss: 0.1224 saved\n",
      "Epoch [120/350] - Loss: 0.1222; val_loss: 0.1224 \n",
      "Epoch [121/350] - Loss: 0.1220; val_loss: 0.1223 \n",
      "Epoch [122/350] - Loss: 0.1217; val_loss: 0.1213 \n",
      "Epoch [123/350] - Loss: 0.1212; val_loss: 0.1218 saved\n",
      "Epoch [124/350] - Loss: 0.1209; val_loss: 0.1214 saved\n",
      "Epoch [125/350] - Loss: 0.1262; val_loss: 0.1215 \n",
      "Epoch [126/350] - Loss: 0.1225; val_loss: 0.1212 \n",
      "Epoch [127/350] - Loss: 0.1209; val_loss: 0.1226 saved\n",
      "Epoch [128/350] - Loss: 0.1208; val_loss: 0.1200 saved\n",
      "Epoch [129/350] - Loss: 0.1204; val_loss: 0.1200 saved\n",
      "Epoch [130/350] - Loss: 0.1609; val_loss: 0.1522 \n",
      "Epoch [131/350] - Loss: 0.1432; val_loss: 0.1438 \n",
      "Epoch [132/350] - Loss: 0.1330; val_loss: 0.1275 \n",
      "Epoch [133/350] - Loss: 0.1286; val_loss: 0.1275 \n",
      "Epoch [134/350] - Loss: 0.1292; val_loss: 0.1259 \n",
      "Epoch [135/350] - Loss: 0.1269; val_loss: 0.1252 \n",
      "Epoch [136/350] - Loss: 0.1393; val_loss: 0.1309 \n",
      "Epoch [137/350] - Loss: 0.1281; val_loss: 0.1276 \n",
      "Epoch [138/350] - Loss: 0.1263; val_loss: 0.1255 \n",
      "Epoch [139/350] - Loss: 0.1249; val_loss: 0.1245 \n",
      "Epoch [140/350] - Loss: 0.1311; val_loss: 0.1268 \n",
      "Epoch [141/350] - Loss: 0.1312; val_loss: 0.1272 \n",
      "Epoch [142/350] - Loss: 0.1269; val_loss: 0.1251 \n",
      "Epoch [143/350] - Loss: 0.1251; val_loss: 0.1238 \n",
      "Epoch [144/350] - Loss: 0.1242; val_loss: 0.1240 \n",
      "Epoch [145/350] - Loss: 0.1232; val_loss: 0.1231 \n",
      "Epoch [146/350] - Loss: 0.1228; val_loss: 0.1212 \n",
      "Epoch [147/350] - Loss: 0.1230; val_loss: 0.1256 \n",
      "Epoch [148/350] - Loss: 0.1218; val_loss: 0.1214 \n",
      "Epoch [149/350] - Loss: 0.1217; val_loss: 0.1223 \n",
      "Epoch [150/350] - Loss: 0.1217; val_loss: 0.1215 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m     10\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Evint\\Documents\\Projects\\Functional-Group-Analysis\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Evint\\Documents\\Projects\\Functional-Group-Analysis\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[108], line 33\u001b[0m, in \u001b[0;36mTokenizerANN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 33\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(encoded)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoded\n",
      "File \u001b[1;32mc:\\Users\\Evint\\Documents\\Projects\\Functional-Group-Analysis\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Evint\\Documents\\Projects\\Functional-Group-Analysis\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Evint\\Documents\\Projects\\Functional-Group-Analysis\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Evint\\Documents\\Projects\\Functional-Group-Analysis\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Evint\\Documents\\Projects\\Functional-Group-Analysis\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Evint\\Documents\\Projects\\Functional-Group-Analysis\\venv\\lib\\site-packages\\torch\\nn\\modules\\activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Evint\\Documents\\Projects\\Functional-Group-Analysis\\venv\\lib\\site-packages\\torch\\nn\\functional.py:1500\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_loss = 1e5\n",
    "hist_loss = []\n",
    "hist_val_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    loss_acc = []\n",
    "    for inputs, targets in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(torch.tensor(inputs, dtype=torch.float32))\n",
    "        loss = criterion(outputs, targets.to(torch.float32))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_acc.append(loss.item())\n",
    "    \n",
    "    epoch_loss = np.mean(loss_acc)\n",
    "    val_outputs = model(torch.tensor(tensor_X_test, dtype=torch.float32))\n",
    "    val_loss = criterion(val_outputs, tensor_y_test)\n",
    "    hist_loss.append(epoch_loss.item())\n",
    "    hist_val_loss.append(val_loss.item())\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f}; val_loss: {val_loss:.4f}\", end=\" \")\n",
    "    if min_loss > epoch_loss:\n",
    "        torch.save(model.state_dict(), \"tokenizer_ann_with_decoder.pth\")\n",
    "        min_loss = epoch_loss\n",
    "        print(\"saved\")\n",
    "    else:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x273bfcbb640>]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist_loss)\n",
    "plt.plot(hist_val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
